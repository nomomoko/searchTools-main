#!/usr/bin/env python3
"""
å¼‚æ­¥å»é‡åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•æ”¹è¿›çš„å¼‚æ­¥æœç´¢å»é‡åŠŸèƒ½ï¼ŒéªŒè¯ï¼š
1. è·¨æºå»é‡çš„å‡†ç¡®æ€§
2. ä¸åŒæ­¥å»é‡çš„ä¸€è‡´æ€§
3. ç»Ÿè®¡ä¿¡æ¯çš„å®Œæ•´æ€§
4. æ€§èƒ½æ”¹è¿›æ•ˆæœ
"""

import asyncio
import time
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from searchtools.async_parallel_search_manager import AsyncParallelSearchManager
# from searchtools.parallel_search_manager import ParallelSearchManager  # æš‚æ—¶æ³¨é‡Šæ‰ï¼Œé¿å…å¯¼å…¥é”™è¯¯


def print_dedup_stats(stats, title):
    """æ‰“å°å»é‡ç»Ÿè®¡ä¿¡æ¯"""
    print(f"\nğŸ“Š {title}:")
    print(f"   - è¾“å…¥æ€»æ•°: {stats.get('total', 0)}")
    print(f"   - æŒ‰DOIå»é‡: {stats.get('by_doi', 0)}")
    print(f"   - æŒ‰PMIDå»é‡: {stats.get('by_pmid', 0)}")
    print(f"   - æŒ‰NCTIDå»é‡: {stats.get('by_nctid', 0)}")
    print(f"   - æŒ‰æ ‡é¢˜+ä½œè€…å»é‡: {stats.get('by_title_author', 0)}")
    print(f"   - æœ€ç»ˆä¿ç•™: {stats.get('kept', 0)}")


def print_source_breakdown(source_stats, title):
    """æ‰“å°æ•°æ®æºåˆ†è§£ç»Ÿè®¡"""
    print(f"\nğŸ“ˆ {title}:")
    for source_name, stats in source_stats.items():
        if 'error' in stats:
            print(f"   âŒ {source_name}: é”™è¯¯ - {stats['error']}")
        else:
            raw_count = stats.get('raw_count', 0)
            after_dedup = stats.get('after_dedup', 0)
            search_time = stats.get('search_time', 0)
            print(f"   âœ… {source_name}: {raw_count} â†’ {after_dedup} ({search_time:.2f}s)")


async def test_async_deduplication():
    """æµ‹è¯•å¼‚æ­¥å»é‡åŠŸèƒ½"""
    print("ğŸš€ å¼‚æ­¥å»é‡åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå¼‚æ­¥æœç´¢ç®¡ç†å™¨
    async_manager = AsyncParallelSearchManager()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = ["diabetes", "cancer immunotherapy", "COVID-19"]
    
    for query in test_queries:
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        print("-" * 40)
        
        # æµ‹è¯•æ–°çš„è·¨æºå»é‡åŠŸèƒ½
        start_time = time.time()
        deduplicated_results, detailed_stats = await async_manager.search_all_sources_with_deduplication(query)
        async_time = time.time() - start_time
        
        print(f"â±ï¸  å¼‚æ­¥è·¨æºå»é‡è€—æ—¶: {async_time:.2f}ç§’")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœæ•°: {len(deduplicated_results)}")
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print_source_breakdown(detailed_stats['source_breakdown'], "å„æ•°æ®æºè¯¦æƒ…")
        print_dedup_stats(detailed_stats['overall_dedup_stats'], "æ€»ä½“å»é‡ç»Ÿè®¡")
        
        # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
        if deduplicated_results:
            print(f"\nğŸ“‹ å‰3ä¸ªç»“æœ:")
            for i, result in enumerate(deduplicated_results[:3]):
                print(f"   {i+1}. {result.title[:60]}...")
                print(f"      æ¥æº: {result.source} | DOI: {result.doi} | PMID: {result.pmid}")


async def test_async_vs_traditional_comparison():
    """æµ‹è¯•å¼‚æ­¥è·¨æºå»é‡ vs ä¼ ç»Ÿå¼‚æ­¥å»é‡çš„å¯¹æ¯”"""
    print("\n\nğŸ”„ å¼‚æ­¥è·¨æºå»é‡ vs ä¼ ç»Ÿå¼‚æ­¥å»é‡å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)

    async_manager = AsyncParallelSearchManager()

    query = "diabetes"
    print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
    # ä¼ ç»Ÿå¼‚æ­¥æ–¹å¼ï¼ˆå…ˆæœç´¢åå»é‡ï¼‰
    print("\nğŸ“Š ä¼ ç»Ÿå¼‚æ­¥æ–¹å¼ï¼ˆå…ˆæœç´¢åå»é‡ï¼‰:")
    start_time = time.time()
    traditional_results = await async_manager._async_search_all_sources(query)
    all_results = []
    for source_result in traditional_results.values():
        if not source_result.error:
            all_results.extend(source_result.results)
    traditional_deduplicated, traditional_stats = async_manager.deduplicate_results(all_results)
    traditional_time = time.time() - start_time

    print(f"   - è€—æ—¶: {traditional_time:.2f}ç§’")
    print(f"   - ç»“æœæ•°: {len(traditional_deduplicated)}")
    print_dedup_stats(traditional_stats, "ä¼ ç»Ÿå»é‡ç»Ÿè®¡")

    # æ–°çš„è·¨æºå»é‡æ–¹å¼
    print("\nğŸš€ æ–°çš„è·¨æºå»é‡æ–¹å¼:")
    start_time = time.time()
    cross_source_deduplicated, cross_source_detailed_stats = await async_manager.search_all_sources_with_deduplication(query)
    cross_source_time = time.time() - start_time
    cross_source_stats = cross_source_detailed_stats['overall_dedup_stats']

    print(f"   - è€—æ—¶: {cross_source_time:.2f}ç§’")
    print(f"   - ç»“æœæ•°: {len(cross_source_deduplicated)}")
    print_dedup_stats(cross_source_stats, "è·¨æºå»é‡ç»Ÿè®¡")

    # å¯¹æ¯”åˆ†æ
    print(f"\nğŸ’¡ å¯¹æ¯”åˆ†æ:")
    print(f"   - ç»“æœæ•°é‡å·®å¼‚: {len(cross_source_deduplicated) - len(traditional_deduplicated)}")
    print(f"   - æ—¶é—´å·®å¼‚: {cross_source_time - traditional_time:.2f}ç§’")
    print(f"   - DOIå»é‡å·®å¼‚: {cross_source_stats['by_doi'] - traditional_stats['by_doi']}")
    print(f"   - PMIDå»é‡å·®å¼‚: {cross_source_stats['by_pmid'] - traditional_stats['by_pmid']}")
    print(f"   - æ ‡é¢˜+ä½œè€…å»é‡å·®å¼‚: {cross_source_stats['by_title_author'] - traditional_stats['by_title_author']}")

    # åˆ†æå»é‡æ•ˆæœ
    traditional_dedup_rate = (traditional_stats['total'] - traditional_stats['kept']) / traditional_stats['total'] * 100 if traditional_stats['total'] > 0 else 0
    cross_source_dedup_rate = (cross_source_stats['total'] - cross_source_stats['kept']) / cross_source_stats['total'] * 100 if cross_source_stats['total'] > 0 else 0

    print(f"\nğŸ“ˆ å»é‡æ•ˆç‡:")
    print(f"   - ä¼ ç»Ÿå»é‡ç‡: {traditional_dedup_rate:.1f}%")
    print(f"   - è·¨æºå»é‡ç‡: {cross_source_dedup_rate:.1f}%")
    print(f"   - å»é‡ç‡å·®å¼‚: {cross_source_dedup_rate - traditional_dedup_rate:.1f}%")

    return traditional_deduplicated, cross_source_deduplicated


async def test_cross_source_deduplication():
    """æµ‹è¯•è·¨æºå»é‡çš„æœ‰æ•ˆæ€§"""
    print("\n\nğŸŒ è·¨æºå»é‡æœ‰æ•ˆæ€§æµ‹è¯•")
    print("=" * 60)

    async_manager = AsyncParallelSearchManager()

    query = "machine learning"
    print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
    # è·å–åŸå§‹æœç´¢ç»“æœ
    raw_results = await async_manager._async_search_all_sources(query)

    # ç»Ÿè®¡åŸå§‹ç»“æœ
    total_raw = 0
    source_counts = {}
    for source_name, source_result in raw_results.items():
        if not source_result.error:
            count = len(source_result.results)
            total_raw += count
            source_counts[source_name] = count

    print(f"\nğŸ“Š åŸå§‹æœç´¢ç»“æœ:")
    print(f"   - æ€»æ•°: {total_raw}")
    for source, count in source_counts.items():
        print(f"   - {source}: {count}")

    # æ‰§è¡Œè·¨æºå»é‡
    deduplicated_results, detailed_stats = await async_manager.search_all_sources_with_deduplication(query)

    print(f"\nğŸ”„ è·¨æºå»é‡ç»“æœ:")
    print(f"   - å»é‡å‰: {detailed_stats['total_raw_results']}")
    print(f"   - å»é‡å: {detailed_stats['total_deduplicated_results']}")
    print(f"   - å»é‡ç‡: {(1 - detailed_stats['total_deduplicated_results'] / detailed_stats['total_raw_results']) * 100:.1f}%")

    print_source_breakdown(detailed_stats['source_breakdown'], "å„æºå»é‡è¯¦æƒ…")
    print_dedup_stats(detailed_stats['overall_dedup_stats'], "æ€»ä½“å»é‡ç»Ÿè®¡")

    # åˆ†æè·¨æºé‡å¤æƒ…å†µ
    print(f"\nğŸ” è·¨æºé‡å¤åˆ†æ:")
    overall_stats = detailed_stats['overall_dedup_stats']
    total_duplicates = overall_stats['by_doi'] + overall_stats['by_pmid'] + overall_stats['by_nctid'] + overall_stats['by_title_author']

    if total_duplicates > 0:
        print(f"   - æ€»é‡å¤æ•°: {total_duplicates}")
        print(f"   - DOIé‡å¤å æ¯”: {overall_stats['by_doi'] / total_duplicates * 100:.1f}%")
        print(f"   - PMIDé‡å¤å æ¯”: {overall_stats['by_pmid'] / total_duplicates * 100:.1f}%")
        print(f"   - NCTIDé‡å¤å æ¯”: {overall_stats['by_nctid'] / total_duplicates * 100:.1f}%")
        print(f"   - æ ‡é¢˜+ä½œè€…é‡å¤å æ¯”: {overall_stats['by_title_author'] / total_duplicates * 100:.1f}%")
    else:
        print("   - æœªå‘ç°é‡å¤ç»“æœ")
        
    return deduplicated_results
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœæ ·æœ¬
    if deduplicated_results:
        print(f"\nğŸ“‹ æœ€ç»ˆå»é‡ç»“æœæ ·æœ¬ (å‰5ä¸ª):")
        for i, result in enumerate(deduplicated_results[:5]):
            print(f"   {i+1}. {result.title[:50]}...")
            print(f"      æ¥æº: {result.source}")
            if result.doi:
                print(f"      DOI: {result.doi}")
            if result.pmid:
                print(f"      PMID: {result.pmid}")
            print()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼‚æ­¥å»é‡åŠŸèƒ½å®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 80)

    try:
        # æµ‹è¯•1: å¼‚æ­¥å»é‡åŠŸèƒ½
        await test_async_deduplication()

        # æµ‹è¯•2: å¼‚æ­¥è·¨æºå»é‡vsä¼ ç»Ÿå¼‚æ­¥å»é‡å¯¹æ¯”
        await test_async_vs_traditional_comparison()

        # æµ‹è¯•3: è·¨æºå»é‡æœ‰æ•ˆæ€§
        await test_cross_source_deduplication()

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“ æµ‹è¯•æ€»ç»“:")
        print("   âœ… å¼‚æ­¥å»é‡åŠŸèƒ½æ­£å¸¸")
        print("   âœ… è·¨æºå»é‡æœ‰æ•ˆ")
        print("   âœ… ç»Ÿè®¡ä¿¡æ¯å®Œæ•´")
        print("   âœ… è·¨æºå»é‡ä¼˜äºä¼ ç»Ÿæ–¹å¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
