"""
å¼‚æ­¥ç‰ˆæœ¬çš„å¹¶è¡Œæœç´¢ç®¡ç†å™¨
å†…éƒ¨ä½¿ç”¨å¼‚æ­¥å¹¶å‘ï¼Œå¤–éƒ¨ä¿æŒåŒæ­¥æ¥å£
"""

import asyncio
import time
import logging
from typing import List, Dict, Set, Tuple, Any

# å¯¼å…¥å¼‚æ­¥æœç´¢åŒ…è£…å™¨
from .searchAPIchoose.async_europe_pmc import AsyncEuropePMCAPIWrapper
from .searchAPIchoose.async_biorxiv import AsyncBioRxivAPIWrapper
from .searchAPIchoose.async_medrxiv import AsyncMedRxivAPIWrapper
from .searchAPIchoose.async_clinical_trials import AsyncClinicalTrialsAPIWrapper
from .searchAPIchoose.async_semantic import AsyncSemanticScholarWrapper
from .searchAPIchoose.async_pubmed import AsyncPubMedAPIWrapper

# å¯¼å…¥æ•°æ®æ¨¡å‹
from .models import SearchResult, SourceSearchResult

logger = logging.getLogger(__name__)


class AsyncParallelSearchManager:
    """å¼‚æ­¥ç‰ˆæœ¬çš„å¤šæºå¹¶è¡Œæœç´¢ç®¡ç†å™¨"""

    def __init__(self):
        from .search_config import get_config

        config = get_config()

        # åˆå§‹åŒ–æ‰€æœ‰å¼‚æ­¥æœç´¢æº
        self.async_sources = {}

        # Europe PMC
        api_config = config.get_api_config("europe_pmc")
        if api_config.enabled:
            self.async_sources["epmc"] = AsyncEuropePMCAPIWrapper(
                page_size=api_config.max_results)
            logger.info("[AsyncParallelSearch] Europe PMC enabled")

        # BioRxiv
        api_config = config.get_api_config("biorxiv")
        if api_config.enabled:
            self.async_sources["biorxiv"] = AsyncBioRxivAPIWrapper()
            logger.info("[AsyncParallelSearch] BioRxiv enabled")

        # MedRxiv
        api_config = config.get_api_config("medrxiv")
        if api_config.enabled:
            self.async_sources["medrxiv"] = AsyncMedRxivAPIWrapper()
            logger.info("[AsyncParallelSearch] MedRxiv enabled")

        # Clinical Trials
        api_config = config.get_api_config("clinical_trials")
        if api_config.enabled:
            self.async_sources[
                "clinical_trials"] = AsyncClinicalTrialsAPIWrapper()
            logger.info("[AsyncParallelSearch] Clinical Trials enabled")

        # Semantic Scholar
        api_config = config.get_api_config("semantic_scholar")
        # è·å– API keyï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        semantic_api_key = getattr(config, "semantic_scholar_api_key",
                                   "") or ""
        if api_config.enabled and semantic_api_key:
            self.async_sources[
                "semantic_scholar"] = AsyncSemanticScholarWrapper(
                    api_key=semantic_api_key, limit=api_config.max_results)
            logger.info("[AsyncParallelSearch] Semantic Scholar enabled")
        elif api_config.enabled:
            logger.warning(
                "[AsyncParallelSearch] Semantic Scholar enabled but no API key found"
            )

        # PubMed
        api_config = config.get_api_config("pubmed")
        if api_config.enabled:
            self.async_sources["pubmed"] = AsyncPubMedAPIWrapper(
                top_k_results=api_config.max_results)
            logger.info("[AsyncParallelSearch] PubMed enabled")

    def search_all_sources(
            self,
            query: str,
            excluded_sources: List[str] = None,
            max_workers: int = None) -> Dict[str, SourceSearchResult]:
        """
        å¹¶è¡Œæœç´¢æ‰€æœ‰æº - åŒæ­¥æ¥å£ï¼ˆå†…éƒ¨ä½¿ç”¨å¼‚æ­¥ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            excluded_sources: è¦æ’é™¤çš„æºåˆ—è¡¨
            max_workers: è¿™ä¸ªå‚æ•°åœ¨å¼‚æ­¥ç‰ˆæœ¬ä¸­è¢«å¿½ç•¥

        Returns:
            Dict[source_name, SourceSearchResult]
        """
        # ä½¿ç”¨ asyncio.run è¿è¡Œå¼‚æ­¥ç‰ˆæœ¬
        return asyncio.run(
            self._async_search_all_sources(query, excluded_sources))

    async def _async_search_all_sources(self,
                                        query: str,
                                        excluded_sources: List[str] = None
                                        ) -> Dict[str, SourceSearchResult]:
        """
        å¼‚æ­¥å¹¶è¡Œæœç´¢æ‰€æœ‰æº

        Args:
            query: æœç´¢æŸ¥è¯¢
            excluded_sources: è¦æ’é™¤çš„æºåˆ—è¡¨

        Returns:
            Dict[source_name, SourceSearchResult]
        """
        excluded = set(excluded_sources or [])
        sources_to_search = {
            k: v
            for k, v in self.async_sources.items() if k not in excluded
        }

        # åˆ›å»ºæ‰€æœ‰æœç´¢ä»»åŠ¡
        tasks = []
        for source_name, wrapper in sources_to_search.items():
            task = self._search_single_source_async(source_name, wrapper,
                                                    query)
            tasks.append((source_name, task))

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æœç´¢
        results = {}
        search_results = await asyncio.gather(*[task for _, task in tasks],
                                              return_exceptions=True)

        # å¤„ç†ç»“æœ
        for (source_name, _), result in zip(tasks, search_results):
            if isinstance(result, Exception):
                logger.error(
                    f"[AsyncParallelSearch] {source_name} failed: {result}")
                results[source_name] = SourceSearchResult(
                    source=source_name,
                    query=query,
                    results=[],
                    results_count=0,
                    error=str(result),
                )
            else:
                results[source_name] = result
                logger.info(
                    f"[AsyncParallelSearch] {source_name} completed: {result.results_count} results"
                )

        return results

    async def _search_single_source_async(self, source: str, wrapper: Any,
                                          query: str) -> SourceSearchResult:
        """å¼‚æ­¥æœç´¢å•ä¸ªæº"""
        start_time = time.time()

        try:
            # è°ƒç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            results = await wrapper.run(query)
            search_time = time.time() - start_time

            # è½¬æ¢ç»“æœæ ¼å¼
            formatted_results = []
            for paper in results:
                # æ„å»ºç»Ÿä¸€çš„ç»“æœæ ¼å¼
                result = {
                    "title": paper.get("title", ""),
                    "authors": paper.get("authors", ""),
                    "journal": paper.get("journal", ""),
                    "year": paper.get("year", ""),
                    "citations": paper.get("citations", 0),
                    "doi": paper.get("doi", ""),
                    "pmid": paper.get("pmid", ""),
                    "pmcid": paper.get("pmcid", ""),
                    "published_date": paper.get("published_date", ""),
                    "url": paper.get("url", ""),
                    "abstract": paper.get("abstract", ""),
                    "source": source,
                }

                # æ·»åŠ æºç‰¹å®šçš„å­—æ®µ
                if source == "clinical_trials":
                    result["nct_id"] = paper.get("nct_id", "")
                    result["status"] = paper.get("status", "")
                    result["conditions"] = paper.get("conditions", "")
                    result["interventions"] = paper.get("interventions", "")

                formatted_results.append(result)

            return SourceSearchResult(
                source=source,
                query=query,
                results=formatted_results,
                results_count=len(formatted_results),
                search_time=search_time,
            )

        except Exception as e:
            search_time = time.time() - start_time
            error_msg = str(e)

            # å¤„ç†ç‰¹æ®Šé”™è¯¯
            if "403" in error_msg or "Forbidden" in error_msg:
                logger.warning(
                    f"[AsyncParallelSearch] {source} returned 403 Forbidden")
                error_msg = "Source temporarily disabled due to 403 Forbidden error"
            else:
                logger.error(
                    f"[AsyncParallelSearch] Error searching {source}: {e}")

            return SourceSearchResult(
                source=source,
                query=query,
                results=[],
                results_count=0,
                search_time=search_time,
                error=error_msg,
            )

    def deduplicate_results(
        self,
        new_results: List[SearchResult],
        existing_identifiers: Set[Tuple[str, str]] = None,
    ) -> Tuple[List[SearchResult], Dict[str, int]]:
        """
        è·¨æºå¤šå±‚çº§å»é‡ï¼ˆä¸åŒæ­¥ç‰ˆè§„åˆ™å¯¹é½ï¼‰ï¼š
        1) DOI â†’ 2) PMID â†’ 3) NCTIDï¼ˆä¸´åºŠè¯•éªŒï¼‰â†’ 4) æ ‡é¢˜+ç¬¬ä¸€ä½œè€…

        Args:
            new_results: æ–°çš„æœç´¢ç»“æœ
            existing_identifiers: å·²å­˜åœ¨çš„æ ‡è¯†ç¬¦é›†åˆï¼ˆè·¨æºå…±äº«ï¼Œé”®ä¸åŒ…å« sourceï¼‰

        Returns:
            (å»é‡åçš„ç»“æœåˆ—è¡¨, å»é‡ç»Ÿè®¡)
        """
        # åˆå§‹åŒ–å·²è§æ ‡è¯†é›†åˆ
        if existing_identifiers is None:
            seen_identifiers: Set[Tuple[str, str]] = set()
        else:
            seen_identifiers = existing_identifiers.copy()

        deduplicated: List[SearchResult] = []
        stats: Dict[str, int] = {
            "total": len(new_results),
            "by_doi": 0,
            "by_pmid": 0,
            "by_nctid": 0,
            "by_title_author": 0,
            "kept": 0,
        }

        # å·¥å…·å‡½æ•°ï¼šæå–ç¬¬ä¸€ä½œè€…ï¼ˆä»å­—ç¬¦ä¸²åˆ‡åˆ†ï¼‰
        def _extract_first_author(authors: str) -> str:
            if not authors:
                return ""
            # å¸¸è§åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€andã€&
            import re

            parts = re.split(r";|,|\band\b|&", authors, flags=re.IGNORECASE)
            first = parts[0].strip() if parts else ""
            return first

        # å·¥å…·å‡½æ•°ï¼šè§„èŒƒåŒ–æ ‡é¢˜
        def _normalize_title(title: str) -> str:
            return (title or "").lower().strip()

        for result in new_results:
            is_duplicate = False

            # 1. DOIï¼ˆç»Ÿä¸€å°å†™ï¼‰
            if result.doi:
                doi_key = ("doi", result.doi.lower().strip())
                if doi_key in seen_identifiers:
                    stats["by_doi"] += 1
                    is_duplicate = True
                else:
                    pass

            # 2. PMID
            if (not is_duplicate) and result.pmid:
                pmid_key = ("pmid", result.pmid.strip())
                if pmid_key in seen_identifiers:
                    stats["by_pmid"] += 1
                    is_duplicate = True

            # 3. NCTIDï¼ˆä¸´åºŠè¯•éªŒï¼‰â€”â€” å…¼å®¹å±æ€§å nct_id / nctid
            nctid_value = getattr(result, "nct_id", "") or getattr(result, "nctid", "")
            if (not is_duplicate) and nctid_value:
                nctid_key = ("nctid", str(nctid_value).strip())
                if nctid_key in seen_identifiers:
                    stats["by_nctid"] += 1
                    is_duplicate = True

            # 4. æ ‡é¢˜ + ç¬¬ä¸€ä½œè€…ï¼ˆåœ¨æ—  DOI ä¸”æ—  PMID çš„æƒ…å†µä¸‹ä½œä¸ºå…œåº•ï¼‰
            if (not is_duplicate) and (not result.doi) and (not result.pmid):
                first_author = _extract_first_author(result.authors)
                title_norm = _normalize_title(result.title)
                ta_key = ("title_author", f"{title_norm}_{first_author.lower().strip()}")
                if ta_key in seen_identifiers:
                    stats["by_title_author"] += 1
                    is_duplicate = True

            if not is_duplicate:
                # ä¿ç•™ç»“æœ
                deduplicated.append(result)
                stats["kept"] += 1

                # å†™å…¥å·²è§é›†åˆï¼ˆæŒ‰å¼ºé”®ä¼˜å…ˆï¼‰
                if result.doi:
                    seen_identifiers.add(("doi", result.doi.lower().strip()))
                if result.pmid:
                    seen_identifiers.add(("pmid", result.pmid.strip()))
                if nctid_value:
                    seen_identifiers.add(("nctid", str(nctid_value).strip()))
                if (not result.doi) and (not result.pmid):
                    first_author = _extract_first_author(result.authors)
                    title_norm = _normalize_title(result.title)
                    seen_identifiers.add(("title_author", f"{title_norm}_{first_author.lower().strip()}"))

        return deduplicated, stats

    def _is_similar_title(self, title1: str, title2: str) -> bool:
        """ç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶è½¬æ¢ä¸ºå°å†™
        import re

        clean1 = re.sub(r"[^\w\s]", "", title1.lower())
        clean2 = re.sub(r"[^\w\s]", "", title2.lower())

        # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥
        return clean1 == clean2


# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªåˆ«å
ParallelSearchManager = AsyncParallelSearchManager

if __name__ == "__main__":
    """æµ‹è¯• AsyncParallelSearchManager çš„å¼‚æ­¥æœç´¢å’Œå»é‡åŠŸèƒ½"""

    # ä¿®å¤å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os

    sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

    from src.searchtools.models import SearchResult

    async def test_async_search_and_deduplication():
        """æµ‹è¯•å¼‚æ­¥æœç´¢å’Œå»é‡åŠŸèƒ½"""
        print("ğŸš€ æµ‹è¯• AsyncParallelSearchManager å¼‚æ­¥æœç´¢å’Œå»é‡åŠŸèƒ½")
        print("=" * 60)

        # åˆ›å»ºæœç´¢ç®¡ç†å™¨å®ä¾‹
        search_manager = AsyncParallelSearchManager()

        # æ˜¾ç¤ºå¯ç”¨çš„æœç´¢æº
        print(f"âœ… å¯ç”¨çš„æœç´¢æºæ•°é‡: {len(search_manager.async_sources)}")
        for source_name in search_manager.async_sources.keys():
            print(f"   - {source_name}")

        if not search_manager.async_sources:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æœç´¢æºï¼Œè¯·æ£€æŸ¥é…ç½®")
            return

        # æµ‹è¯•æŸ¥è¯¢
        test_query = "cancer immunotherapy"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {test_query}")

        try:
            # æ‰§è¡Œå¼‚æ­¥æœç´¢
            print("\nâ³ å¼€å§‹å¼‚æ­¥æœç´¢...")
            start_time = time.time()

            results = await search_manager._async_search_all_sources(test_query
                                                                     )

            search_time = time.time() - start_time
            print(f"âœ… æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.2f}ç§’")

            # æ˜¾ç¤ºæœç´¢ç»“æœç»Ÿè®¡
            print("\nğŸ“Š æœç´¢ç»“æœç»Ÿè®¡:")
            total_results = 0
            for source_name, source_result in results.items():
                if hasattr(source_result, "error") and source_result.error:
                    print(f"   {source_name}: âŒ {source_result.error}")
                else:
                    result_count = getattr(source_result, "results_count", 0)
                    search_time = getattr(source_result, "search_time", 0)
                    total_results += result_count
                    print(
                        f"   {source_name}: âœ… {result_count} ä¸ªç»“æœ (è€—æ—¶: {search_time:.2f}s)"
                    )

            print(f"\n   æ€»è®¡: {total_results} ä¸ªç»“æœ")

            # æ”¶é›†æ‰€æœ‰ç»“æœè¿›è¡Œå»é‡
            print("\nğŸ”„ å¼€å§‹å»é‡å¤„ç†...")

            # è½¬æ¢ä¸º SearchResult å¯¹è±¡åˆ—è¡¨
            all_results = []
            for source_name, source_result in results.items():
                if hasattr(source_result, "error") and source_result.error:
                    continue

                for result_data in getattr(source_result, "results", []):
                    search_result = SearchResult(
                        title=result_data.get("title", ""),
                        authors=result_data.get("authors", ""),
                        journal=result_data.get("journal", ""),
                        year=result_data.get("year", ""),
                        citations=result_data.get("citations", 0),
                        doi=result_data.get("doi", ""),
                        pmid=result_data.get("pmid", ""),
                        pmcid=result_data.get("pmcid", ""),
                        published_date=result_data.get("published_date", ""),
                        url=result_data.get("url", ""),
                        abstract=result_data.get("abstract", ""),
                        source=source_name,
                    )
                    all_results.append(search_result)

            print(f"   å»é‡å‰ç»“æœæ•°é‡: {len(all_results)}")

            # æ‰§è¡Œå»é‡
            deduplicated_results, duplicate_stats = search_manager.deduplicate_results(
                all_results)

            print(f"   å»é‡åç»“æœæ•°é‡: {len(deduplicated_results)}")
            print("   é‡å¤ç»“æœç»Ÿè®¡:")
            print(f"     - è¾“å…¥æ€»æ•°: {duplicate_stats['total']}")
            print(f"     - æŒ‰DOIé‡å¤: {duplicate_stats['by_doi']}")
            print(f"     - æŒ‰PMIDé‡å¤: {duplicate_stats['by_pmid']}")
            print(f"     - æŒ‰NCTIDé‡å¤: {duplicate_stats['by_nctid']}")
            print(f"     - æŒ‰æ ‡é¢˜+ä½œè€…é‡å¤: {duplicate_stats['by_title_author']}")
            print(f"     - ä¿ç•™æ•°é‡: {duplicate_stats['kept']}")

            # æ˜¾ç¤ºå»é‡åçš„å‰å‡ ä¸ªç»“æœ
            if deduplicated_results:
                print("\nğŸ“‹ å»é‡åçš„å‰3ä¸ªç»“æœ:")
                for i, result in enumerate(deduplicated_results[:3]):
                    print(f"   {i + 1}. {result.title}")
                    print(f"      ä½œè€…: {result.authors}")
                    print(f"      æœŸåˆŠ: {result.journal}")
                    print(f"      å¹´ä»½: {result.year}")
                    print(f"      DOI: {result.doi}")
                    print(f"      æ¥æº: {result.source}")
                    print()

            print("âœ… æµ‹è¯•å®Œæˆï¼")

        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback

            traceback.print_exc()

    # è¿è¡Œæµ‹è¯•
    try:
        asyncio.run(test_async_search_and_deduplication())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
