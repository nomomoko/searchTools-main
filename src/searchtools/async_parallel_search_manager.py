"""
å¼‚æ­¥ç‰ˆæœ¬çš„å¹¶è¡Œæœç´¢ç®¡ç†å™¨
å†…éƒ¨ä½¿ç”¨å¼‚æ­¥å¹¶å‘ï¼Œå¤–éƒ¨ä¿æŒåŒæ­¥æ¥å£
"""

import asyncio
import time
import logging
from typing import List, Dict, Set, Tuple, Any

# å¯¼å…¥å¼‚æ­¥æœç´¢åŒ…è£…å™¨
from .searchAPIchoose.async_europe_pmc import AsyncEuropePMCAPIWrapper
from .searchAPIchoose.async_biorxiv import AsyncBioRxivAPIWrapper
from .searchAPIchoose.async_medrxiv import AsyncMedRxivAPIWrapper
from .searchAPIchoose.async_clinical_trials import AsyncClinicalTrialsAPIWrapper
from .searchAPIchoose.async_semantic import AsyncSemanticScholarWrapper
from .searchAPIchoose.async_pubmed import AsyncPubMedAPIWrapper

# å¯¼å…¥æ•°æ®æ¨¡å‹
from .models import SearchResult, SourceSearchResult

logger = logging.getLogger(__name__)


def _extract_first_author(authors: str) -> str:
    """
    æå–ç¬¬ä¸€ä½œè€…å§“å

    Args:
        authors: ä½œè€…å­—ç¬¦ä¸²ï¼Œå¯èƒ½æ˜¯å¤šç§æ ¼å¼

    Returns:
        ç¬¬ä¸€ä½œè€…å§“åï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not authors:
        return ""

    # å¤„ç†ä¸åŒçš„ä½œè€…åˆ†éš”ç¬¦
    separators = [";", ",", " and ", " & ", "\n"]
    first_author = authors

    for sep in separators:
        if sep in authors:
            first_author = authors.split(sep)[0].strip()
            break

    # æ¸…ç†ä½œè€…åå­—ï¼ˆç§»é™¤å¸¸è§çš„åç¼€ï¼‰
    suffixes_to_remove = [" Jr.", " Sr.", " III", " II", " PhD", " MD", " Dr."]
    for suffix in suffixes_to_remove:
        if first_author.endswith(suffix):
            first_author = first_author[:-len(suffix)].strip()

    return first_author


def _normalize_title(title: str) -> str:
    """
    æ ‡å‡†åŒ–æ ‡é¢˜ç”¨äºå»é‡æ¯”è¾ƒ

    Args:
        title: åŸå§‹æ ‡é¢˜

    Returns:
        æ ‡å‡†åŒ–åçš„æ ‡é¢˜
    """
    if not title:
        return ""

    # è½¬æ¢ä¸ºå°å†™å¹¶ç§»é™¤å¤šä½™ç©ºæ ¼
    normalized = title.lower().strip()

    # ç§»é™¤å¸¸è§çš„æ ‡ç‚¹ç¬¦å·
    import re
    normalized = re.sub(r'[^\w\s]', ' ', normalized)

    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    normalized = re.sub(r'\s+', ' ', normalized).strip()

    return normalized


class AsyncParallelSearchManager:
    """å¼‚æ­¥ç‰ˆæœ¬çš„å¤šæºå¹¶è¡Œæœç´¢ç®¡ç†å™¨"""

    def __init__(self):
        from .search_config import get_config

        config = get_config()

        # åˆå§‹åŒ–æ‰€æœ‰å¼‚æ­¥æœç´¢æº
        self.async_sources = {}

        # Europe PMC
        api_config = config.get_api_config("europe_pmc")
        if api_config.enabled:
            self.async_sources["epmc"] = AsyncEuropePMCAPIWrapper(
                page_size=api_config.max_results)
            logger.info("[AsyncParallelSearch] Europe PMC enabled")

        # BioRxiv
        api_config = config.get_api_config("biorxiv")
        if api_config.enabled:
            self.async_sources["biorxiv"] = AsyncBioRxivAPIWrapper()
            logger.info("[AsyncParallelSearch] BioRxiv enabled")

        # MedRxiv
        api_config = config.get_api_config("medrxiv")
        if api_config.enabled:
            self.async_sources["medrxiv"] = AsyncMedRxivAPIWrapper()
            logger.info("[AsyncParallelSearch] MedRxiv enabled")

        # Clinical Trials
        api_config = config.get_api_config("clinical_trials")
        if api_config.enabled:
            self.async_sources[
                "clinical_trials"] = AsyncClinicalTrialsAPIWrapper()
            logger.info("[AsyncParallelSearch] Clinical Trials enabled")

        # Semantic Scholar
        api_config = config.get_api_config("semantic_scholar")
        # è·å– API keyï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        semantic_api_key = getattr(config, "semantic_scholar_api_key",
                                   "") or ""
        if api_config.enabled and semantic_api_key:
            self.async_sources[
                "semantic_scholar"] = AsyncSemanticScholarWrapper(
                    api_key=semantic_api_key, limit=api_config.max_results)
            logger.info("[AsyncParallelSearch] Semantic Scholar enabled")
        elif api_config.enabled:
            logger.warning(
                "[AsyncParallelSearch] Semantic Scholar enabled but no API key found"
            )

        # PubMed
        api_config = config.get_api_config("pubmed")
        if api_config.enabled:
            self.async_sources["pubmed"] = AsyncPubMedAPIWrapper(
                top_k_results=api_config.max_results)
            logger.info("[AsyncParallelSearch] PubMed enabled")

    def search_all_sources(
            self,
            query: str,
            excluded_sources: List[str] = None,
            max_workers: int = None) -> Dict[str, SourceSearchResult]:
        """
        å¹¶è¡Œæœç´¢æ‰€æœ‰æº - åŒæ­¥æ¥å£ï¼ˆå†…éƒ¨ä½¿ç”¨å¼‚æ­¥ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            excluded_sources: è¦æ’é™¤çš„æºåˆ—è¡¨
            max_workers: è¿™ä¸ªå‚æ•°åœ¨å¼‚æ­¥ç‰ˆæœ¬ä¸­è¢«å¿½ç•¥

        Returns:
            Dict[source_name, SourceSearchResult]
        """
        # ä½¿ç”¨ asyncio.run è¿è¡Œå¼‚æ­¥ç‰ˆæœ¬
        return asyncio.run(
            self._async_search_all_sources(query, excluded_sources))

    async def _async_search_all_sources(self,
                                        query: str,
                                        excluded_sources: List[str] = None
                                        ) -> Dict[str, SourceSearchResult]:
        """
        å¼‚æ­¥å¹¶è¡Œæœç´¢æ‰€æœ‰æº

        Args:
            query: æœç´¢æŸ¥è¯¢
            excluded_sources: è¦æ’é™¤çš„æºåˆ—è¡¨

        Returns:
            Dict[source_name, SourceSearchResult]
        """
        excluded = set(excluded_sources or [])
        sources_to_search = {
            k: v
            for k, v in self.async_sources.items() if k not in excluded
        }

        # åˆ›å»ºæ‰€æœ‰æœç´¢ä»»åŠ¡
        tasks = []
        for source_name, wrapper in sources_to_search.items():
            task = self._search_single_source_async(source_name, wrapper,
                                                    query)
            tasks.append((source_name, task))

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æœç´¢
        results = {}
        search_results = await asyncio.gather(*[task for _, task in tasks],
                                              return_exceptions=True)

        # å¤„ç†ç»“æœ
        for (source_name, _), result in zip(tasks, search_results):
            if isinstance(result, Exception):
                logger.error(
                    f"[AsyncParallelSearch] {source_name} failed: {result}")
                results[source_name] = SourceSearchResult(
                    source=source_name,
                    query=query,
                    results=[],
                    results_count=0,
                    error=str(result),
                )
            else:
                results[source_name] = result
                logger.info(
                    f"[AsyncParallelSearch] {source_name} completed: {result.results_count} results"
                )

        return results

    async def _search_single_source_async(self, source: str, wrapper: Any,
                                          query: str) -> SourceSearchResult:
        """å¼‚æ­¥æœç´¢å•ä¸ªæº"""
        start_time = time.time()

        try:
            # è°ƒç”¨å¼‚æ­¥æœç´¢æ–¹æ³•
            results = await wrapper.run(query)
            search_time = time.time() - start_time

            # è½¬æ¢ç»“æœæ ¼å¼ä¸ºSearchResultå¯¹è±¡
            formatted_results = []
            for paper in results:
                # æ„å»ºSearchResultå¯¹è±¡
                search_result = SearchResult(
                    title=paper.get("title", ""),
                    authors=paper.get("authors", ""),
                    journal=paper.get("journal", ""),
                    year=paper.get("year", ""),
                    citations=paper.get("citations", 0),
                    doi=paper.get("doi", ""),
                    pmid=paper.get("pmid", ""),
                    pmcid=paper.get("pmcid", ""),
                    published_date=paper.get("published_date", ""),
                    url=paper.get("url", ""),
                    abstract=paper.get("abstract", ""),
                    source=source,
                )

                # æ·»åŠ æºç‰¹å®šçš„å­—æ®µ
                if source == "clinical_trials":
                    search_result.nct_id = paper.get("nct_id", "")
                    search_result.status = paper.get("status", "")
                    search_result.conditions = paper.get("conditions", "")
                    search_result.interventions = paper.get("interventions", "")

                formatted_results.append(search_result)

            return SourceSearchResult(
                source=source,
                query=query,
                results=formatted_results,
                results_count=len(formatted_results),
                search_time=search_time,
            )

        except Exception as e:
            search_time = time.time() - start_time
            error_msg = str(e)

            # å¤„ç†ç‰¹æ®Šé”™è¯¯
            if "403" in error_msg or "Forbidden" in error_msg:
                logger.warning(
                    f"[AsyncParallelSearch] {source} returned 403 Forbidden")
                error_msg = "Source temporarily disabled due to 403 Forbidden error"
            else:
                logger.error(
                    f"[AsyncParallelSearch] Error searching {source}: {e}")

            return SourceSearchResult(
                source=source,
                query=query,
                results=[],
                results_count=0,
                search_time=search_time,
                error=error_msg,
            )

    def deduplicate_results(
        self,
        new_results: List[SearchResult],
        existing_identifiers: Set[Tuple[str, str]] = None,
    ) -> Tuple[List[SearchResult], Dict[str, int]]:
        """
        è·¨æºå¤šå±‚çº§å»é‡ï¼ˆä¸åŒæ­¥ç‰ˆæœ¬å®Œå…¨ä¸€è‡´ï¼‰ï¼š
        1) DOI â†’ 2) PMID â†’ 3) NCTIDï¼ˆä¸´åºŠè¯•éªŒï¼‰â†’ 4) æ ‡é¢˜+ç¬¬ä¸€ä½œè€…

        Args:
            new_results: æ–°çš„æœç´¢ç»“æœï¼ˆSearchResultå¯¹è±¡åˆ—è¡¨ï¼‰
            existing_identifiers: å·²å­˜åœ¨çš„æ ‡è¯†ç¬¦é›†åˆï¼ˆè·¨æºå…±äº«ï¼‰

        Returns:
            (å»é‡åçš„ç»“æœåˆ—è¡¨, å»é‡ç»Ÿè®¡)
        """
        # åˆå§‹åŒ–å·²è§æ ‡è¯†é›†åˆ
        if existing_identifiers is None:
            seen_identifiers: Set[Tuple[str, str]] = set()
        else:
            seen_identifiers = existing_identifiers.copy()

        logger.info(f"[AsyncDeduplication] Starting with {len(new_results)} new results")
        logger.info(f"[AsyncDeduplication] Existing identifiers count: {len(seen_identifiers)}")

        if len(seen_identifiers) > 0 and len(seen_identifiers) < 10:
            # Log first few identifiers for debugging
            sample_ids = list(seen_identifiers)[:5]
            logger.info(f"[AsyncDeduplication] Sample existing identifiers: {sample_ids}")

        deduplicated: List[SearchResult] = []
        stats: Dict[str, int] = {
            "total": len(new_results),
            "by_doi": 0,
            "by_pmid": 0,
            "by_nctid": 0,
            "by_title_author": 0,
            "kept": 0,
        }

        # å·¥å…·å‡½æ•°ï¼šæå–ç¬¬ä¸€ä½œè€…ï¼ˆä»å­—ç¬¦ä¸²åˆ‡åˆ†ï¼‰
        def _extract_first_author(authors: str) -> str:
            if not authors:
                return ""
            # å¸¸è§åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€andã€&
            import re

            parts = re.split(r";|,|\band\b|&", authors, flags=re.IGNORECASE)
            first = parts[0].strip() if parts else ""
            return first

        # å·¥å…·å‡½æ•°ï¼šè§„èŒƒåŒ–æ ‡é¢˜
        def _normalize_title(title: str) -> str:
            return (title or "").lower().strip()

        for idx, result in enumerate(new_results):
            is_duplicate = False
            duplicate_reason = None

            # 1. ä¼˜å…ˆæ£€æŸ¥DOIï¼ˆç»Ÿä¸€å°å†™ï¼‰
            if result.doi:
                doi_key = ("doi", result.doi.lower().strip())
                if doi_key in seen_identifiers:
                    is_duplicate = True
                    duplicate_reason = f"DOI: {result.doi}"
                    stats["by_doi"] += 1

            # 2. æ£€æŸ¥PMID
            if not is_duplicate and result.pmid:
                pmid_key = ("pmid", result.pmid.strip())
                if pmid_key in seen_identifiers:
                    is_duplicate = True
                    duplicate_reason = f"PMID: {result.pmid}"
                    stats["by_pmid"] += 1

            # 3. æ£€æŸ¥NCT IDï¼ˆä¸´åºŠè¯•éªŒï¼‰- å…¼å®¹å¤šç§å±æ€§å
            nctid_value = getattr(result, "nct_id", "") or getattr(result, "nctid", "")
            if not is_duplicate and nctid_value:
                nctid_key = ("nctid", str(nctid_value).strip())
                if nctid_key in seen_identifiers:
                    is_duplicate = True
                    duplicate_reason = f"NCTID: {nctid_value}"
                    stats["by_nctid"] += 1

            # 4. æ£€æŸ¥æ ‡é¢˜å’Œä½œè€…ç»„åˆï¼ˆåœ¨æ— DOIä¸”æ— PMIDçš„æƒ…å†µä¸‹ä½œä¸ºå…œåº•ï¼‰
            if not is_duplicate and not result.doi and not result.pmid:
                first_author = _extract_first_author(result.authors)
                title_normalized = _normalize_title(result.title)
                identifier = f"{title_normalized}_{first_author.lower().strip()}"
                ta_key = ("title_author", identifier)

                if ta_key in seen_identifiers:
                    is_duplicate = True
                    duplicate_reason = f"Title+Author: {result.title[:50]}..."
                    stats["by_title_author"] += 1

            # Log first few duplicates for debugging
            if is_duplicate and stats["total"] - stats["kept"] < 3:
                logger.info(f"[AsyncDeduplication] Filtered out duplicate #{idx}: {duplicate_reason}")

            # å¦‚æœä¸æ˜¯é‡å¤ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if not is_duplicate:
                deduplicated.append(result)
                stats["kept"] += 1

                # æ›´æ–°æ ‡è¯†ç¬¦é›†åˆï¼ˆæŒ‰å¼ºé”®ä¼˜å…ˆï¼‰
                if result.doi:
                    seen_identifiers.add(("doi", result.doi.lower().strip()))
                if result.pmid:
                    seen_identifiers.add(("pmid", result.pmid.strip()))
                if nctid_value:
                    seen_identifiers.add(("nctid", str(nctid_value).strip()))
                if not result.doi and not result.pmid:
                    first_author = _extract_first_author(result.authors)
                    title_normalized = _normalize_title(result.title)
                    identifier = f"{title_normalized}_{first_author.lower().strip()}"
                    seen_identifiers.add(("title_author", identifier))

        logger.info(f"[AsyncDeduplication] Completed: kept {stats['kept']} out of {stats['total']} results")
        return deduplicated, stats

    async def search_all_sources_with_deduplication(
        self,
        query: str,
        excluded_sources: List[str] = None
    ) -> Tuple[List[SearchResult], Dict[str, Any]]:
        """
        æ‰§è¡Œè·¨æºæœç´¢å¹¶è¿›è¡Œç»Ÿä¸€å»é‡

        Args:
            query: æœç´¢æŸ¥è¯¢
            excluded_sources: è¦æ’é™¤çš„æºåˆ—è¡¨

        Returns:
            (å»é‡åçš„ç»“æœåˆ—è¡¨, è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯)
        """
        # æ‰§è¡Œå¼‚æ­¥æœç´¢
        source_results = await self._async_search_all_sources(query, excluded_sources)

        # æ”¶é›†æ‰€æœ‰ç»“æœå¹¶è¿›è¡Œè·¨æºå»é‡
        all_results = []
        source_stats = {}
        seen_identifiers = set()

        # æŒ‰æºå¤„ç†ç»“æœï¼Œå®ç°çœŸæ­£çš„è·¨æºå»é‡
        for source_name, source_result in source_results.items():
            if source_result.error:
                logger.warning(f"[AsyncCrossSourceDedup] {source_name} failed: {source_result.error}")
                source_stats[source_name] = {
                    "raw_count": 0,
                    "after_dedup": 0,
                    "error": source_result.error
                }
                continue

            # å¯¹å½“å‰æºçš„ç»“æœè¿›è¡Œå»é‡
            source_deduplicated, source_dedup_stats = self.deduplicate_results(
                source_result.results, seen_identifiers
            )

            # æ›´æ–°seen_identifiersä»¥å½±å“åç»­æºçš„å»é‡
            for result in source_deduplicated:
                if result.doi:
                    seen_identifiers.add(("doi", result.doi.lower().strip()))
                if result.pmid:
                    seen_identifiers.add(("pmid", result.pmid.strip()))
                nctid_value = getattr(result, "nct_id", "") or getattr(result, "nctid", "")
                if nctid_value:
                    seen_identifiers.add(("nctid", str(nctid_value).strip()))
                if not result.doi and not result.pmid:
                    first_author = _extract_first_author(result.authors)
                    title_normalized = _normalize_title(result.title)
                    identifier = f"{title_normalized}_{first_author.lower().strip()}"
                    seen_identifiers.add(("title_author", identifier))

            all_results.extend(source_deduplicated)
            source_stats[source_name] = {
                "raw_count": source_result.results_count,
                "after_dedup": len(source_deduplicated),
                "dedup_stats": source_dedup_stats,
                "search_time": source_result.search_time
            }

            logger.info(f"[AsyncCrossSourceDedup] {source_name}: {source_result.results_count} â†’ {len(source_deduplicated)} after dedup")

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        total_stats = {
            "query": query,
            "total_sources": len(source_results),
            "successful_sources": len([s for s in source_stats.values() if "error" not in s]),
            "total_raw_results": sum(s.get("raw_count", 0) for s in source_stats.values()),
            "total_deduplicated_results": len(all_results),
            "source_breakdown": source_stats,
            "overall_dedup_stats": {
                "total": sum(s.get("dedup_stats", {}).get("total", 0) for s in source_stats.values()),
                "by_doi": sum(s.get("dedup_stats", {}).get("by_doi", 0) for s in source_stats.values()),
                "by_pmid": sum(s.get("dedup_stats", {}).get("by_pmid", 0) for s in source_stats.values()),
                "by_nctid": sum(s.get("dedup_stats", {}).get("by_nctid", 0) for s in source_stats.values()),
                "by_title_author": sum(s.get("dedup_stats", {}).get("by_title_author", 0) for s in source_stats.values()),
                "kept": len(all_results)
            }
        }

        logger.info(f"[AsyncCrossSourceDedup] Final results: {total_stats['total_raw_results']} â†’ {len(all_results)} after cross-source deduplication")

        return all_results, total_stats

    def search_all_sources_with_deduplication_sync(
        self,
        query: str,
        excluded_sources: List[str] = None
    ) -> Tuple[List[SearchResult], Dict[str, Any]]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„è·¨æºæœç´¢å’Œå»é‡ï¼ˆç”¨äºæµ‹è¯•å’Œå…¼å®¹æ€§ï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            excluded_sources: è¦æ’é™¤çš„æºåˆ—è¡¨

        Returns:
            (å»é‡åçš„ç»“æœåˆ—è¡¨, è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯)
        """
        # ä½¿ç”¨asyncio.runæ‰§è¡Œå¼‚æ­¥ç‰ˆæœ¬
        return asyncio.run(self.search_all_sources_with_deduplication(query, excluded_sources))

    def _is_similar_title(self, title1: str, title2: str) -> bool:
        """ç®€å•çš„æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å¹¶è½¬æ¢ä¸ºå°å†™
        import re

        clean1 = re.sub(r"[^\w\s]", "", title1.lower())
        clean2 = re.sub(r"[^\w\s]", "", title2.lower())

        # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥
        return clean1 == clean2


# ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºä¸€ä¸ªåˆ«å
ParallelSearchManager = AsyncParallelSearchManager

if __name__ == "__main__":
    """æµ‹è¯• AsyncParallelSearchManager çš„å¼‚æ­¥æœç´¢å’Œå»é‡åŠŸèƒ½"""

    # ä¿®å¤å¯¼å…¥é—®é¢˜ - ä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os

    sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", ".."))

    from src.searchtools.models import SearchResult

    async def test_async_search_and_deduplication():
        """æµ‹è¯•æ”¹è¿›çš„å¼‚æ­¥æœç´¢å’Œè·¨æºå»é‡åŠŸèƒ½"""
        print("ğŸš€ æµ‹è¯•æ”¹è¿›çš„ AsyncParallelSearchManager åŠŸèƒ½")
        print("=" * 60)

        # åˆ›å»ºæœç´¢ç®¡ç†å™¨å®ä¾‹
        search_manager = AsyncParallelSearchManager()

        # æ˜¾ç¤ºå¯ç”¨çš„æœç´¢æº
        print(f"âœ… å¯ç”¨çš„æœç´¢æºæ•°é‡: {len(search_manager.async_sources)}")
        for source_name in search_manager.async_sources.keys():
            print(f"   - {source_name}")

        if not search_manager.async_sources:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æœç´¢æºï¼Œè¯·æ£€æŸ¥é…ç½®")
            return

        # æµ‹è¯•æŸ¥è¯¢
        test_query = "diabetes"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {test_query}")

        try:
            # æµ‹è¯•æ–°çš„è·¨æºå»é‡åŠŸèƒ½
            print("\nğŸš€ æµ‹è¯•è·¨æºå»é‡åŠŸèƒ½:")
            start_time = time.time()

            deduplicated_results, detailed_stats = search_manager.search_all_sources_with_deduplication(test_query)
            search_time = time.time() - start_time

            print(f"âœ… è·¨æºæœç´¢å’Œå»é‡å®Œæˆï¼Œè€—æ—¶: {search_time:.2f}ç§’")

            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   - æŸ¥è¯¢: {detailed_stats['query']}")
            print(f"   - æ€»æ•°æ®æº: {detailed_stats['total_sources']}")
            print(f"   - æˆåŠŸæ•°æ®æº: {detailed_stats['successful_sources']}")
            print(f"   - åŸå§‹ç»“æœæ€»æ•°: {detailed_stats['total_raw_results']}")
            print(f"   - å»é‡åç»“æœæ•°: {detailed_stats['total_deduplicated_results']}")

            print(f"\nğŸ“ˆ å„æ•°æ®æºè¯¦æƒ…:")
            for source_name, source_stat in detailed_stats['source_breakdown'].items():
                if 'error' in source_stat:
                    print(f"   âŒ {source_name}: é”™è¯¯ - {source_stat['error']}")
                else:
                    print(f"   âœ… {source_name}: {source_stat['raw_count']} â†’ {source_stat['after_dedup']} "
                          f"({source_stat['search_time']:.2f}s)")

            print(f"\nğŸ”„ æ€»ä½“å»é‡ç»Ÿè®¡:")
            overall_stats = detailed_stats['overall_dedup_stats']
            print(f"   - è¾“å…¥æ€»æ•°: {overall_stats['total']}")
            print(f"   - æŒ‰DOIå»é‡: {overall_stats['by_doi']}")
            print(f"   - æŒ‰PMIDå»é‡: {overall_stats['by_pmid']}")
            print(f"   - æŒ‰NCTIDå»é‡: {overall_stats['by_nctid']}")
            print(f"   - æŒ‰æ ‡é¢˜+ä½œè€…å»é‡: {overall_stats['by_title_author']}")
            print(f"   - æœ€ç»ˆä¿ç•™: {overall_stats['kept']}")

            # æ˜¾ç¤ºè·¨æºå»é‡åçš„ç»“æœ
            if deduplicated_results:
                print(f"\nğŸ“‹ è·¨æºå»é‡åçš„å‰5ä¸ªç»“æœ:")
                for i, result in enumerate(deduplicated_results[:5]):
                    print(f"   {i + 1}. {result.title}")
                    print(f"      ä½œè€…: {result.authors}")
                    print(f"      æœŸåˆŠ: {result.journal}")
                    print(f"      å¹´ä»½: {result.year}")
                    print(f"      DOI: {result.doi}")
                    print(f"      æ¥æº: {result.source}")
                    print()

            print("âœ… è·¨æºå»é‡æµ‹è¯•å®Œæˆï¼")

            # å¯¹æ¯”æµ‹è¯•ï¼šå±•ç¤ºæ”¹è¿›å‰åçš„å·®å¼‚
            print("\nğŸ”„ å¯¹æ¯”æµ‹è¯• - ä¼ ç»Ÿæ–¹å¼ vs æ”¹è¿›æ–¹å¼:")

            # ä¼ ç»Ÿæ–¹å¼
            print("ğŸ“Š ä¼ ç»Ÿæ–¹å¼ï¼ˆå…ˆæœç´¢åå»é‡ï¼‰:")
            start_time = time.time()
            traditional_results = search_manager.search_all_sources(test_query)
            all_results = []
            for source_result in traditional_results.values():
                if not source_result.error:
                    all_results.extend(source_result.results)
            traditional_deduplicated, traditional_stats = search_manager.deduplicate_results(all_results)
            traditional_time = time.time() - start_time

            print(f"   - è€—æ—¶: {traditional_time:.2f}ç§’")
            print(f"   - ç»“æœæ•°: {len(traditional_deduplicated)}")
            print(f"   - å»é‡ç»Ÿè®¡: DOI:{traditional_stats['by_doi']}, PMID:{traditional_stats['by_pmid']}, "
                  f"NCTID:{traditional_stats['by_nctid']}, Title+Author:{traditional_stats['by_title_author']}")

            print("ğŸš€ æ”¹è¿›æ–¹å¼ï¼ˆè·¨æºå»é‡ï¼‰:")
            print(f"   - è€—æ—¶: {search_time:.2f}ç§’")
            print(f"   - ç»“æœæ•°: {len(deduplicated_results)}")
            print(f"   - å»é‡ç»Ÿè®¡: DOI:{overall_stats['by_doi']}, PMID:{overall_stats['by_pmid']}, "
                  f"NCTID:{overall_stats['by_nctid']}, Title+Author:{overall_stats['by_title_author']}")

            print(f"\nğŸ’¡ æ”¹è¿›æ•ˆæœ:")
            print(f"   - ç»“æœæ•°é‡å·®å¼‚: {len(deduplicated_results) - len(traditional_deduplicated)}")
            print(f"   - æ—¶é—´å·®å¼‚: {search_time - traditional_time:.2f}ç§’")

            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback

            traceback.print_exc()

    # è¿è¡Œæµ‹è¯•
    try:
        asyncio.run(test_async_search_and_deduplication())
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
