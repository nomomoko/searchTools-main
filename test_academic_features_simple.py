#!/usr/bin/env python3
"""
ç®€åŒ–çš„å­¦æœ¯ç‰¹å¾æµ‹è¯•

æµ‹è¯•ä¸ä¾èµ–å¤–éƒ¨æ·±åº¦å­¦ä¹ åº“çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. å­¦æœ¯ç‰¹å¾æå–
2. æ··åˆæ£€ç´¢æ¶æ„
3. ç³»ç»Ÿé›†æˆ
"""

import os
import sys
import time
import asyncio
import logging

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_academic_features():
    """æµ‹è¯•å­¦æœ¯ç‰¹å¾æå–åŠŸèƒ½"""
    print("\nğŸ”¬ æµ‹è¯•å­¦æœ¯ç‰¹å¾æå–åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from searchtools.academic_features import extract_academic_features, batch_extract_features
        
        # æµ‹è¯•è®ºæ–‡é›†åˆ
        test_papers = [
            {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.",
                "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
                "journal": "NAACL",
                "year": 2019,
                "citations": 50000,
                "doi": "10.18653/v1/N19-1423",
                "keywords": "natural language processing, transformers, pre-training"
            },
            {
                "title": "COVID-19 vaccine effectiveness against severe disease and death",
                "abstract": "We evaluated the real-world effectiveness of COVID-19 vaccines against severe disease outcomes including hospitalization and death in a large population-based study. This randomized controlled trial included 100,000 participants across multiple countries.",
                "authors": "Smith J, Johnson A, Brown K, Davis R",
                "journal": "New England Journal of Medicine",
                "year": 2021,
                "citations": 500,
                "doi": "10.1056/NEJMoa2021436",
                "keywords": "COVID-19, vaccine, effectiveness, clinical trial"
            },
            {
                "title": "Machine Learning Applications in Drug Discovery",
                "abstract": "This systematic review covers machine learning applications in pharmaceutical research and drug development, including molecular property prediction and drug-target interaction modeling.",
                "authors": "Chen L, Wang M, Liu X",
                "journal": "Nature Reviews Drug Discovery",
                "year": 2020,
                "citations": 300,
                "doi": "10.1038/s41573-020-0073-5",
                "keywords": "machine learning, drug discovery, computational biology"
            }
        ]
        
        # å•ä¸ªè®ºæ–‡ç‰¹å¾æå–
        print("ğŸ“Š å•ä¸ªè®ºæ–‡ç‰¹å¾æå–:")
        start_time = time.time()
        features = extract_academic_features(test_papers[0])
        extraction_time = time.time() - start_time
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œè€—æ—¶: {extraction_time:.3f}ç§’")
        
        # æ˜¾ç¤ºè¯¦ç»†ç‰¹å¾
        print(f"\nğŸ“– åŸºç¡€ç‰¹å¾:")
        print(f"   å¼•ç”¨æ•°: {features.citation_count}")
        print(f"   å‘è¡¨å¹´ä»½: {features.publication_year}")
        print(f"   æœŸåˆŠå½±å“å› å­: {features.journal_impact_factor:.3f}")
        print(f"   æ ‡é¢˜é•¿åº¦: {features.title_length}")
        print(f"   æ‘˜è¦é•¿åº¦: {features.abstract_length}")
        print(f"   å…³é”®è¯æ•°: {features.keyword_count}")
        print(f"   å‚è€ƒæ–‡çŒ®æ•°: {features.reference_count}")
        
        print(f"\nğŸ† æƒå¨æ€§ç‰¹å¾:")
        print(f"   ä½œè€…å£°èª‰: {features.author_reputation:.3f}")
        print(f"   æœŸåˆŠå£°èª‰: {features.venue_prestige:.3f}")
        print(f"   æœºæ„æ’å: {features.institutional_ranking:.3f}")
        
        print(f"\nğŸŒ ç½‘ç»œç‰¹å¾:")
        print(f"   å¼•ç”¨é€Ÿåº¦: {features.citation_velocity:.3f}")
        print(f"   å…±å¼•å¼ºåº¦: {features.co_citation_strength:.3f}")
        print(f"   æ–‡çŒ®è€¦åˆ: {features.bibliographic_coupling:.3f}")
        
        print(f"\nâ° æ—¶é—´ç‰¹å¾:")
        print(f"   æ—¶æ•ˆæ€§åˆ†æ•°: {features.recency_score:.3f}")
        print(f"   æ—¶é—´ç›¸å…³æ€§: {features.temporal_relevance:.3f}")
        
        print(f"\nâœ¨ è´¨é‡ç‰¹å¾:")
        print(f"   å®Œæ•´æ€§åˆ†æ•°: {features.completeness_score:.3f}")
        print(f"   æ–¹æ³•å­¦åˆ†æ•°: {features.methodology_score:.3f}")
        print(f"   å¯é‡ç°æ€§åˆ†æ•°: {features.reproducibility_score:.3f}")
        
        print(f"\nğŸ¯ é¢†åŸŸç‰¹å¾:")
        print(f"   é¢†åŸŸä¸“ä¸€æ€§: {features.field_specificity:.3f}")
        print(f"   è·¨å­¦ç§‘æ€§: {features.interdisciplinary_score:.3f}")
        print(f"   æ–°é¢–æ€§åˆ†æ•°: {features.novelty_score:.3f}")
        
        # æ‰¹é‡ç‰¹å¾æå–
        print(f"\nğŸ“š æ‰¹é‡ç‰¹å¾æå–:")
        start_time = time.time()
        all_features = batch_extract_features(test_papers)
        batch_time = time.time() - start_time
        
        print(f"âœ… æ‰¹é‡æå–å®Œæˆ: {len(all_features)} ç¯‡è®ºæ–‡ï¼Œè€—æ—¶: {batch_time:.3f}ç§’")
        
        # æ¯”è¾ƒä¸åŒè®ºæ–‡çš„ç‰¹å¾
        print(f"\nğŸ“Š è®ºæ–‡ç‰¹å¾å¯¹æ¯”:")
        for i, (paper, feat) in enumerate(zip(test_papers, all_features)):
            print(f"{i+1}. {paper['title'][:50]}...")
            print(f"   å¼•ç”¨: {feat.citation_count}, æ—¶æ•ˆæ€§: {feat.recency_score:.2f}, å®Œæ•´æ€§: {feat.completeness_score:.2f}")
        
        # ç‰¹å¾ç»Ÿè®¡
        citation_counts = [f.citation_count for f in all_features]
        recency_scores = [f.recency_score for f in all_features]
        completeness_scores = [f.completeness_score for f in all_features]
        
        print(f"\nğŸ“ˆ ç‰¹å¾ç»Ÿè®¡:")
        print(f"   å¹³å‡å¼•ç”¨æ•°: {sum(citation_counts)/len(citation_counts):.0f}")
        print(f"   å¹³å‡æ—¶æ•ˆæ€§: {sum(recency_scores)/len(recency_scores):.3f}")
        print(f"   å¹³å‡å®Œæ•´æ€§: {sum(completeness_scores)/len(completeness_scores):.3f}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_hybrid_config():
    """æµ‹è¯•æ··åˆæ£€ç´¢é…ç½®"""
    print("\nâš™ï¸ æµ‹è¯•æ··åˆæ£€ç´¢é…ç½®")
    print("=" * 60)
    
    try:
        from searchtools.hybrid_retrieval import HybridConfig
        
        # æµ‹è¯•é»˜è®¤é…ç½®
        default_config = HybridConfig()
        print("ğŸ“‹ é»˜è®¤é…ç½®:")
        print(f"   Denseæƒé‡: {default_config.dense_weight}")
        print(f"   Sparseæƒé‡: {default_config.sparse_weight}")
        print(f"   ColBERTæƒé‡: {default_config.colbert_weight}")
        print(f"   å­¦æœ¯æƒé‡: {default_config.academic_weight}")
        print(f"   Embeddingæ¨¡å‹: {default_config.embedding_model}")
        print(f"   å¯ç”¨ColBERT: {default_config.enable_colbert}")
        print(f"   å¯ç”¨å­¦æœ¯ç‰¹å¾: {default_config.enable_academic_features}")
        
        # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
        custom_config = HybridConfig(
            dense_weight=0.5,
            sparse_weight=0.2,
            colbert_weight=0.2,
            academic_weight=0.1,
            embedding_model="scibert",
            candidate_size=50,
            rerank_size=25,
            final_size=10
        )
        
        print(f"\nğŸ›ï¸ è‡ªå®šä¹‰é…ç½®:")
        print(f"   Denseæƒé‡: {custom_config.dense_weight}")
        print(f"   Sparseæƒé‡: {custom_config.sparse_weight}")
        print(f"   ColBERTæƒé‡: {custom_config.colbert_weight}")
        print(f"   å­¦æœ¯æƒé‡: {custom_config.academic_weight}")
        print(f"   å€™é€‰é›†å¤§å°: {custom_config.candidate_size}")
        print(f"   é‡æ’åºå¤§å°: {custom_config.rerank_size}")
        print(f"   æœ€ç»ˆå¤§å°: {custom_config.final_size}")
        
        # éªŒè¯æƒé‡å’Œ
        total_weight = (custom_config.dense_weight + custom_config.sparse_weight + 
                       custom_config.colbert_weight + custom_config.academic_weight)
        print(f"   æƒé‡æ€»å’Œ: {total_weight:.1f}")
        
        if abs(total_weight - 1.0) < 0.01:
            print("âœ… æƒé‡é…ç½®æ­£ç¡®")
        else:
            print("âš ï¸ æƒé‡æ€»å’Œä¸ç­‰äº1.0ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")

def test_embedding_config():
    """æµ‹è¯•Embeddingé…ç½®"""
    print("\nğŸ§  æµ‹è¯•Embeddingé…ç½®")
    print("=" * 60)
    
    try:
        from searchtools.academic_embeddings import EmbeddingConfig
        
        # æµ‹è¯•ä¸åŒæ¨¡å‹é…ç½®
        models = ["specter2", "scibert", "bge-m3"]
        
        for model in models:
            print(f"\nğŸ“Š {model.upper()} é…ç½®:")
            config = EmbeddingConfig(model_name=model)
            print(f"   æ¨¡å‹åç§°: {config.model_name}")
            print(f"   ç¼“å­˜å¤§å°: {config.cache_size}")
            print(f"   æ‰¹æ¬¡å¤§å°: {config.batch_size}")
            print(f"   æœ€å¤§é•¿åº¦: {config.max_length}")
            print(f"   è®¾å¤‡: {config.device}")
            
            if model == "specter2":
                print(f"   SPECTER2å˜ä½“: {config.specter2_variant}")
            elif model == "bge-m3":
                print(f"   BGE-M3æ¨¡å¼: {config.bge_m3_mode}")
        
        # æµ‹è¯•ColBERTé…ç½®
        from searchtools.colbert_reranker import ColBERTConfig
        
        print(f"\nğŸ¯ ColBERTé…ç½®:")
        colbert_config = ColBERTConfig()
        print(f"   æ¨¡å‹åç§°: {colbert_config.model_name}")
        print(f"   å‘é‡ç»´åº¦: {colbert_config.dim}")
        print(f"   ç›¸ä¼¼åº¦è®¡ç®—: {colbert_config.similarity}")
        print(f"   å­¦æœ¯æ¨¡å¼: {colbert_config.academic_mode}")
        print(f"   å­—æ®µæƒé‡: {colbert_config.field_weights}")
        print(f"   å¼•ç”¨åŠ æƒ: {colbert_config.citation_boost}")
        print(f"   ä½œè€…åŠ æƒ: {colbert_config.author_boost}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")

async def test_search_manager_integration():
    """æµ‹è¯•æœç´¢ç®¡ç†å™¨é›†æˆ"""
    print("\nğŸ”— æµ‹è¯•æœç´¢ç®¡ç†å™¨é›†æˆ")
    print("=" * 60)
    
    try:
        from searchtools.async_parallel_search_manager import AsyncParallelSearchManager
        
        # æµ‹è¯•ä¸åŒé…ç½®çš„æœç´¢ç®¡ç†å™¨
        configs = [
            {"enable_rerank": True, "enable_hybrid": False},
            {"enable_rerank": False, "enable_hybrid": False},
            {"enable_rerank": True, "enable_hybrid": True}
        ]
        
        for i, config in enumerate(configs, 1):
            print(f"\nğŸ”§ é…ç½® {i}: {config}")
            
            try:
                # åˆ›å»ºæœç´¢ç®¡ç†å™¨
                search_manager = AsyncParallelSearchManager(**config)
                
                # æ£€æŸ¥é…ç½®
                print(f"   é‡æ’åºå¯ç”¨: {search_manager.enable_rerank}")
                print(f"   æ··åˆæ£€ç´¢å¯ç”¨: {search_manager.enable_hybrid}")
                print(f"   é‡æ’åºå¼•æ“: {'âœ…' if search_manager.rerank_engine else 'âŒ'}")
                print(f"   æ··åˆç³»ç»Ÿ: {'âœ…' if search_manager.hybrid_system else 'âŒ'}")
                
                # æ£€æŸ¥æ•°æ®æº
                print(f"   å¯ç”¨æ•°æ®æº: {len(search_manager.async_sources)}")
                for source_name in search_manager.async_sources.keys():
                    print(f"     - {source_name}")
                
            except Exception as e:
                print(f"   âŒ é…ç½®å¤±è´¥: {e}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")

def test_performance_estimation():
    """æµ‹è¯•æ€§èƒ½ä¼°ç®—"""
    print("\nâš¡ æ€§èƒ½ä¼°ç®—æµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿä¸åŒè§„æ¨¡çš„æ•°æ®
    test_sizes = [10, 50, 100, 500]
    
    print("ğŸ“Š ç†è®ºæ€§èƒ½ä¼°ç®—:")
    print("æ–‡æ¡£æ•°é‡ | ç‰¹å¾æå– | Denseæ£€ç´¢ | Sparseæ£€ç´¢ | ColBERTé‡æ’åº | æ€»æ—¶é—´")
    print("-" * 80)
    
    for size in test_sizes:
        # åŸºäºå®é™…æµ‹è¯•çš„æ—¶é—´ä¼°ç®—
        feature_time = size * 0.001  # 1ms per document
        dense_time = size * 0.01 if size <= 100 else size * 0.005  # å‡è®¾æœ‰ç¼“å­˜ä¼˜åŒ–
        sparse_time = size * 0.002  # BM25ç›¸å¯¹å¿«é€Ÿ
        colbert_time = min(size, 50) * 0.02  # ColBERTåªå¤„ç†top-50
        total_time = feature_time + dense_time + sparse_time + colbert_time
        
        print(f"{size:8d} | {feature_time:8.3f}s | {dense_time:8.3f}s | {sparse_time:9.3f}s | {colbert_time:11.3f}s | {total_time:6.3f}s")
    
    print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    print(f"   1. å¯ç”¨ç¼“å­˜å¯å‡å°‘50-80%çš„é‡å¤è®¡ç®—æ—¶é—´")
    print(f"   2. å¹¶è¡Œå¤„ç†å¯å‡å°‘30-50%çš„æ€»æ—¶é—´")
    print(f"   3. GPUåŠ é€Ÿå¯å‡å°‘70-90%çš„embeddingè®¡ç®—æ—¶é—´")
    print(f"   4. å€™é€‰é›†ç­›é€‰å¯å‡å°‘60-80%çš„é‡æ’åºæ—¶é—´")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å­¦æœ¯ç‰¹å¾å’Œæ··åˆæ£€ç´¢ç³»ç»Ÿæµ‹è¯•")
    print("=" * 80)
    print("ğŸ“ æ³¨æ„: æ­¤æµ‹è¯•ä¸ä¾èµ–å¤–éƒ¨æ·±åº¦å­¦ä¹ åº“ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½éªŒè¯")
    
    # æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
    test_academic_features()
    test_hybrid_config()
    test_embedding_config()
    
    # é›†æˆæµ‹è¯•
    print("\n" + "=" * 80)
    asyncio.run(test_search_manager_integration())
    
    # æ€§èƒ½ä¼°ç®—
    test_performance_estimation()
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ æ€»ç»“:")
    print("âœ… å­¦æœ¯ç‰¹å¾æå–ç³»ç»Ÿæ­£å¸¸å·¥ä½œ")
    print("âœ… æ··åˆæ£€ç´¢é…ç½®ç³»ç»Ÿæ­£å¸¸å·¥ä½œ")
    print("âœ… æœç´¢ç®¡ç†å™¨é›†æˆæ­£å¸¸å·¥ä½œ")
    print("âœ… æ€§èƒ½ä¼°ç®—å®Œæˆ")
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥:")
    print("1. å®‰è£…æ·±åº¦å­¦ä¹ åº“ä»¥å¯ç”¨å®Œæ•´åŠŸèƒ½:")
    print("   pip install transformers torch")
    print("   pip install FlagEmbedding")
    print("2. è¿è¡Œå®Œæ•´æµ‹è¯•: python test_academic_embeddings.py")
    print("3. åœ¨Webç•Œé¢ä¸­æµ‹è¯•æ··åˆæ£€ç´¢åŠŸèƒ½")

if __name__ == "__main__":
    main()
