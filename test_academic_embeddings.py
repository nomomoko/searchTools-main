#!/usr/bin/env python3
"""
å­¦æœ¯Embeddingå’Œæ··åˆæ£€ç´¢ç³»ç»Ÿæµ‹è¯•

æµ‹è¯•æ–°çš„å­¦æœ¯ä¼˜åŒ–åŠŸèƒ½ï¼š
1. SPECTER2/SciBERT embedding
2. ColBERTé‡æ’åº
3. å­¦æœ¯ç‰¹å¾æå–
4. æ··åˆæ£€ç´¢ç³»ç»Ÿ
5. æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import os
import sys
import time
import asyncio
import logging
from typing import List, Dict

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, 'src')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_academic_embeddings():
    """æµ‹è¯•å­¦æœ¯embeddingåŠŸèƒ½"""
    print("\nğŸ§  æµ‹è¯•å­¦æœ¯EmbeddingåŠŸèƒ½")
    print("=" * 60)
    
    try:
        from searchtools.academic_embeddings import create_academic_embedder
        
        # æµ‹è¯•è®ºæ–‡æ•°æ®
        test_papers = [
            {
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers."
            },
            {
                "title": "Attention Is All You Need",
                "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism."
            },
            {
                "title": "COVID-19 vaccine effectiveness against severe disease",
                "abstract": "We evaluated the effectiveness of COVID-19 vaccines against severe disease outcomes including hospitalization and death in a large population-based study."
            }
        ]
        
        # æµ‹è¯•ä¸åŒçš„embeddingæ¨¡å‹
        models_to_test = ["specter2", "scibert"]  # BGE-M3éœ€è¦é¢å¤–å®‰è£…
        
        for model_name in models_to_test:
            print(f"\nğŸ“Š æµ‹è¯• {model_name.upper()} æ¨¡å‹:")
            
            try:
                # åˆ›å»ºembedder
                embedder = create_academic_embedder(model_name=model_name)
                
                # ç¼–ç è®ºæ–‡
                start_time = time.time()
                embeddings = embedder.encode_papers(test_papers)
                encoding_time = time.time() - start_time
                
                print(f"âœ… ç¼–ç å®Œæˆ: {len(embeddings)} ç¯‡è®ºæ–‡")
                print(f"â±ï¸  ç¼–ç æ—¶é—´: {encoding_time:.3f}ç§’")
                print(f"ğŸ“ å‘é‡ç»´åº¦: {embeddings[0].shape}")
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                sim_12 = embedder.compute_similarity(embeddings[0], embeddings[1])
                sim_13 = embedder.compute_similarity(embeddings[0], embeddings[2])
                sim_23 = embedder.compute_similarity(embeddings[1], embeddings[2])
                
                print(f"ğŸ”— BERT vs Attention: {sim_12:.3f}")
                print(f"ğŸ”— BERT vs COVID: {sim_13:.3f}")
                print(f"ğŸ”— Attention vs COVID: {sim_23:.3f}")
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = embedder.get_stats()
                print(f"ğŸ“ˆ ç»Ÿè®¡: {stats}")
                
            except Exception as e:
                print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥: {e}")
                if "transformers" in str(e):
                    print("ğŸ’¡ æç¤º: å®‰è£…transformersåº“: pip install transformers torch")
                elif "FlagEmbedding" in str(e):
                    print("ğŸ’¡ æç¤º: å®‰è£…FlagEmbeddingåº“: pip install FlagEmbedding")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…")

def test_colbert_reranker():
    """æµ‹è¯•ColBERTé‡æ’åºåŠŸèƒ½"""
    print("\nğŸ¯ æµ‹è¯•ColBERTé‡æ’åºåŠŸèƒ½")
    print("=" * 60)
    
    try:
        from searchtools.colbert_reranker import create_colbert_reranker
        
        # æµ‹è¯•æŸ¥è¯¢å’Œæ–‡æ¡£
        query = "machine learning for drug discovery"
        
        documents = [
            {
                "title": "Deep Learning for Drug Discovery: A Comprehensive Review",
                "abstract": "This review covers the application of deep learning methods in pharmaceutical research, including molecular property prediction and drug-target interaction modeling.",
                "citations": 150,
                "journal": "Nature Reviews Drug Discovery"
            },
            {
                "title": "COVID-19 Vaccine Development Timeline",
                "abstract": "We present a timeline of COVID-19 vaccine development from initial virus sequencing to emergency use authorization.",
                "citations": 80,
                "journal": "Science"
            },
            {
                "title": "Machine Learning Applications in Computational Biology",
                "abstract": "Machine learning has revolutionized computational biology, enabling new approaches to protein structure prediction, genomics, and systems biology.",
                "citations": 200,
                "journal": "Cell"
            },
            {
                "title": "Artificial Intelligence in Healthcare: Current Applications",
                "abstract": "AI technologies are being deployed across healthcare, from diagnostic imaging to clinical decision support systems.",
                "citations": 120,
                "journal": "NEJM"
            }
        ]
        
        try:
            # åˆ›å»ºé‡æ’åºå™¨
            reranker = create_colbert_reranker(academic_mode=True)
            
            # æ‰§è¡Œé‡æ’åº
            start_time = time.time()
            results = reranker.rerank(query, documents, top_k=4)
            rerank_time = time.time() - start_time
            
            print(f"âœ… é‡æ’åºå®Œæˆ: {len(results)} ä¸ªç»“æœ")
            print(f"â±ï¸  é‡æ’åºæ—¶é—´: {rerank_time:.3f}ç§’")
            
            # æ˜¾ç¤ºç»“æœ
            print("\nğŸ“‹ é‡æ’åºç»“æœ:")
            for i, (orig_idx, score, doc) in enumerate(results, 1):
                print(f"{i}. [{score:.3f}] {doc['title'][:60]}...")
                print(f"   æœŸåˆŠ: {doc['journal']}, å¼•ç”¨: {doc['citations']}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = reranker.get_stats()
            print(f"\nğŸ“ˆ ç»Ÿè®¡: {stats}")
            
        except Exception as e:
            print(f"âŒ ColBERTæµ‹è¯•å¤±è´¥: {e}")
            if "colbert" in str(e).lower():
                print("ğŸ’¡ æç¤º: ColBERTåº“æœªå®‰è£…ï¼Œä½¿ç”¨transformerså®ç°")
            elif "transformers" in str(e):
                print("ğŸ’¡ æç¤º: å®‰è£…transformersåº“: pip install transformers torch")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")

def test_academic_features():
    """æµ‹è¯•å­¦æœ¯ç‰¹å¾æå–åŠŸèƒ½"""
    print("\nğŸ”¬ æµ‹è¯•å­¦æœ¯ç‰¹å¾æå–åŠŸèƒ½")
    print("=" * 60)
    
    try:
        from searchtools.academic_features import extract_academic_features
        
        # æµ‹è¯•è®ºæ–‡
        test_paper = {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.",
            "authors": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",
            "journal": "NAACL",
            "year": 2019,
            "citations": 50000,
            "doi": "10.18653/v1/N19-1423",
            "keywords": "natural language processing, transformers, pre-training"
        }
        
        # æå–ç‰¹å¾
        start_time = time.time()
        features = extract_academic_features(test_paper)
        extraction_time = time.time() - start_time
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œè€—æ—¶: {extraction_time:.3f}ç§’")
        
        # æ˜¾ç¤ºç‰¹å¾
        print("\nğŸ“Š å­¦æœ¯ç‰¹å¾:")
        print(f"ğŸ“– åŸºç¡€ç‰¹å¾:")
        print(f"   å¼•ç”¨æ•°: {features.citation_count}")
        print(f"   å‘è¡¨å¹´ä»½: {features.publication_year}")
        print(f"   æœŸåˆŠå½±å“å› å­: {features.journal_impact_factor:.3f}")
        print(f"   æ ‡é¢˜é•¿åº¦: {features.title_length}")
        print(f"   æ‘˜è¦é•¿åº¦: {features.abstract_length}")
        
        print(f"\nğŸ† æƒå¨æ€§ç‰¹å¾:")
        print(f"   ä½œè€…å£°èª‰: {features.author_reputation:.3f}")
        print(f"   æœŸåˆŠå£°èª‰: {features.venue_prestige:.3f}")
        print(f"   æœºæ„æ’å: {features.institutional_ranking:.3f}")
        
        print(f"\nğŸŒ ç½‘ç»œç‰¹å¾:")
        print(f"   å¼•ç”¨é€Ÿåº¦: {features.citation_velocity:.3f}")
        print(f"   å…±å¼•å¼ºåº¦: {features.co_citation_strength:.3f}")
        print(f"   æ–‡çŒ®è€¦åˆ: {features.bibliographic_coupling:.3f}")
        
        print(f"\nâ° æ—¶é—´ç‰¹å¾:")
        print(f"   æ—¶æ•ˆæ€§åˆ†æ•°: {features.recency_score:.3f}")
        print(f"   æ—¶é—´ç›¸å…³æ€§: {features.temporal_relevance:.3f}")
        
        print(f"\nâœ¨ è´¨é‡ç‰¹å¾:")
        print(f"   å®Œæ•´æ€§åˆ†æ•°: {features.completeness_score:.3f}")
        print(f"   æ–¹æ³•å­¦åˆ†æ•°: {features.methodology_score:.3f}")
        print(f"   å¯é‡ç°æ€§åˆ†æ•°: {features.reproducibility_score:.3f}")
        
        print(f"\nğŸ¯ é¢†åŸŸç‰¹å¾:")
        print(f"   é¢†åŸŸä¸“ä¸€æ€§: {features.field_specificity:.3f}")
        print(f"   è·¨å­¦ç§‘æ€§: {features.interdisciplinary_score:.3f}")
        print(f"   æ–°é¢–æ€§åˆ†æ•°: {features.novelty_score:.3f}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")

def test_hybrid_retrieval():
    """æµ‹è¯•æ··åˆæ£€ç´¢ç³»ç»Ÿ"""
    print("\nğŸš€ æµ‹è¯•æ··åˆæ£€ç´¢ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        from searchtools.hybrid_retrieval import create_hybrid_system
        
        # æµ‹è¯•æŸ¥è¯¢
        query = "COVID-19 vaccine effectiveness"
        
        # æµ‹è¯•æ–‡æ¡£é›†åˆ
        documents = [
            {
                "title": "COVID-19 vaccine effectiveness against severe disease and death",
                "abstract": "We evaluated the real-world effectiveness of COVID-19 vaccines against severe disease outcomes in a large population-based study.",
                "authors": "Smith J, Johnson A, Brown K",
                "journal": "New England Journal of Medicine",
                "year": 2021,
                "citations": 500,
                "doi": "10.1056/NEJMoa2021436"
            },
            {
                "title": "Machine Learning for Drug Discovery",
                "abstract": "This review covers machine learning applications in pharmaceutical research and drug development.",
                "authors": "Chen L, Wang M",
                "journal": "Nature Reviews Drug Discovery",
                "year": 2020,
                "citations": 300,
                "doi": "10.1038/s41573-020-0073-5"
            },
            {
                "title": "SARS-CoV-2 variants and vaccine breakthrough infections",
                "abstract": "Analysis of vaccine breakthrough infections caused by different SARS-CoV-2 variants of concern.",
                "authors": "Davis R, Miller S, Wilson T",
                "journal": "Science",
                "year": 2022,
                "citations": 200,
                "doi": "10.1126/science.abm1234"
            },
            {
                "title": "Artificial Intelligence in Healthcare",
                "abstract": "Overview of AI applications in healthcare including diagnostic imaging and clinical decision support.",
                "authors": "Lee H, Kim J",
                "journal": "Nature Medicine",
                "year": 2021,
                "citations": 150,
                "doi": "10.1038/s41591-021-01234-x"
            }
        ]
        
        try:
            # åˆ›å»ºæ··åˆæ£€ç´¢ç³»ç»Ÿ
            hybrid_system = create_hybrid_system(
                embedding_model="specter2",
                enable_colbert=True,
                enable_academic_features=True
            )
            
            # æ‰§è¡Œæ··åˆæ£€ç´¢
            start_time = time.time()
            results = hybrid_system.retrieve_and_rank(query, documents, top_k=4)
            retrieval_time = time.time() - start_time
            
            print(f"âœ… æ··åˆæ£€ç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            print(f"â±ï¸  æ£€ç´¢æ—¶é—´: {retrieval_time:.3f}ç§’")
            
            # æ˜¾ç¤ºç»“æœ
            print("\nğŸ† æ··åˆæ£€ç´¢ç»“æœ:")
            for i, (orig_idx, score, doc) in enumerate(results, 1):
                print(f"{i}. [{score:.3f}] {doc['title']}")
                print(f"   ä½œè€…: {doc['authors']}")
                print(f"   æœŸåˆŠ: {doc['journal']} ({doc['year']})")
                print(f"   å¼•ç”¨: {doc['citations']}")
                print()
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = hybrid_system.get_stats()
            print(f"ğŸ“ˆ ç³»ç»Ÿç»Ÿè®¡:")
            for key, value in stats.items():
                if isinstance(value, dict):
                    print(f"   {key}:")
                    for k, v in value.items():
                        print(f"     {k}: {v}")
                else:
                    print(f"   {key}: {value}")
            
        except Exception as e:
            print(f"âŒ æ··åˆæ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")

async def test_integrated_search():
    """æµ‹è¯•é›†æˆçš„æœç´¢ç³»ç»Ÿ"""
    print("\nğŸŒŸ æµ‹è¯•é›†æˆæœç´¢ç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        from searchtools.async_parallel_search_manager import AsyncParallelSearchManager
        
        # åˆ›å»ºæœç´¢ç®¡ç†å™¨ï¼ˆå¯ç”¨æ··åˆæ£€ç´¢ï¼‰
        search_manager = AsyncParallelSearchManager(
            enable_rerank=True,
            enable_hybrid=True
        )
        
        # æµ‹è¯•æŸ¥è¯¢
        test_query = "COVID-19 vaccine"
        
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: {test_query}")
        print("âš ï¸  æ³¨æ„: è¿™å°†è¿›è¡ŒçœŸå®çš„ç½‘ç»œæœç´¢ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        
        # æ‰§è¡Œæœç´¢
        start_time = time.time()
        results, stats = await search_manager.search_all_sources_with_deduplication(test_query)
        search_time = time.time() - start_time
        
        print(f"âœ… æœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
        print(f"â±ï¸  æ€»è€—æ—¶: {search_time:.2f}ç§’")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
        print(f"   åŸå§‹ç»“æœ: {stats.get('total_raw_results', 0)}")
        print(f"   å»é‡å: {len(results)}")
        print(f"   é‡æ’åºå¯ç”¨: {stats.get('rerank_enabled', False)}")
        print(f"   æ··åˆæ£€ç´¢å¯ç”¨: {stats.get('hybrid_enabled', False)}")
        
        # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
        print(f"\nğŸ† å‰5ä¸ªç»“æœ:")
        for i, result in enumerate(results[:5], 1):
            print(f"{i}. {result.title}")
            print(f"   ä½œè€…: {result.authors}")
            print(f"   æ¥æº: {result.source}")
            if hasattr(result, 'hybrid_score'):
                print(f"   æ··åˆåˆ†æ•°: {result.hybrid_score:.3f}")
            elif hasattr(result, 'final_score'):
                print(f"   æœ€ç»ˆåˆ†æ•°: {result.final_score:.3f}")
            print()
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å­¦æœ¯Embeddingå’Œæ··åˆæ£€ç´¢ç³»ç»Ÿæµ‹è¯•")
    print("=" * 80)
    
    # åŸºç¡€ç»„ä»¶æµ‹è¯•
    test_academic_embeddings()
    test_colbert_reranker()
    test_academic_features()
    test_hybrid_retrieval()
    
    # é›†æˆæµ‹è¯•ï¼ˆå¯é€‰ï¼Œéœ€è¦ç½‘ç»œè¿æ¥ï¼‰
    print("\n" + "=" * 80)
    response = input("æ˜¯å¦è¿›è¡Œé›†æˆæœç´¢æµ‹è¯•ï¼Ÿ(éœ€è¦ç½‘ç»œè¿æ¥ï¼Œå¯èƒ½è¾ƒæ…¢) [y/N]: ")
    if response.lower() in ['y', 'yes']:
        asyncio.run(test_integrated_search())
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ’¡ å®‰è£…å»ºè®®:")
    print("   pip install transformers torch")
    print("   pip install FlagEmbedding  # ç”¨äºBGE-M3")
    print("   pip install colbert-ai     # ç”¨äºå®˜æ–¹ColBERT")

if __name__ == "__main__":
    main()
